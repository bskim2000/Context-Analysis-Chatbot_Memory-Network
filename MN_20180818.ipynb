{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "단어장 크기 : 44 중복없는 단어\n",
      "스토리 길이 : 3 단어\n",
      "질문 : 5 단어\n",
      "학습 스토리 개수: 117\n",
      "-\n",
      "데이터 셋은 다음처럼 구성됨 (스토리, 질의, 답변):\n",
      "(['UNK'], ['안녕', '하세요', '?'], '인사1')\n",
      "-\n",
      "스토리 : 벡터크기 (117, 3)\n",
      "질문 : 벡터크기 (117, 5)\n",
      "답변 : (1 또는 0)로 구성된 벡터 크기 (117,)\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/GPU36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "data = 'aurabot'\n",
    "direc = './'\n",
    "train_stories = get_stories(codecs.open(direc + data + '.txt', 'r', 'utf-8'))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories)))\n",
    "\n",
    "print('-')\n",
    "print('단어장 크기 :', vocab_size, '중복없는 단어')\n",
    "print('스토리 길이 :', story_maxlen, '단어')\n",
    "print('질문 :', query_maxlen, '단어')\n",
    "print('학습 스토리 개수:', len(train_stories))\n",
    "print('-')\n",
    "print('데이터 셋은 다음처럼 구성됨 (스토리, 질의, 답변):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "print('스토리 : 벡터크기', inputs_train.shape)\n",
    "print('질문 : 벡터크기', queries_train.shape)\n",
    "print('답변 : (1 또는 0)로 구성된 벡터 크기', answers_train.shape)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "response = add([match, input_encoded_c])\n",
    "response = Permute((2, 1))(response)\n",
    "\n",
    "answer = concatenate([response, question_encoded])\n",
    "answer = LSTM(32)(answer)\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "117/117 [==============================] - 3s 23ms/step - loss: 2.3999 - acc: 0.6410\n",
      "Epoch 2/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 1.1584 - acc: 0.6838\n",
      "Epoch 3/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 1.0140 - acc: 0.6838\n",
      "Epoch 4/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.9274 - acc: 0.6838\n",
      "Epoch 5/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.8998 - acc: 0.6838\n",
      "Epoch 6/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.8294 - acc: 0.7009\n",
      "Epoch 7/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.7110 - acc: 0.7350\n",
      "Epoch 8/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.6185 - acc: 0.7863\n",
      "Epoch 9/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.5748 - acc: 0.7692\n",
      "Epoch 10/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.4628 - acc: 0.7778\n",
      "Epoch 11/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.3937 - acc: 0.8462\n",
      "Epoch 12/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.3146 - acc: 0.8803\n",
      "Epoch 13/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2781 - acc: 0.9060\n",
      "Epoch 14/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2588 - acc: 0.8889\n",
      "Epoch 15/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2428 - acc: 0.9060\n",
      "Epoch 16/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2724 - acc: 0.8889\n",
      "Epoch 17/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2139 - acc: 0.9145\n",
      "Epoch 18/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2811 - acc: 0.9145\n",
      "Epoch 19/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2052 - acc: 0.9231\n",
      "Epoch 20/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.2011 - acc: 0.9487\n",
      "Epoch 21/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.1780 - acc: 0.9573\n",
      "Epoch 22/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.1628 - acc: 0.9573\n",
      "Epoch 23/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.1347 - acc: 0.9658\n",
      "Epoch 24/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.1304 - acc: 0.9573\n",
      "Epoch 25/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.1541 - acc: 0.9573\n",
      "Epoch 26/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.1208 - acc: 0.9658\n",
      "Epoch 27/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.1414 - acc: 0.9573\n",
      "Epoch 28/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0787 - acc: 0.9829\n",
      "Epoch 29/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0976 - acc: 0.9744\n",
      "Epoch 30/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0786 - acc: 0.9744\n",
      "Epoch 31/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0991 - acc: 0.9829\n",
      "Epoch 32/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0752 - acc: 0.9829\n",
      "Epoch 33/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0667 - acc: 0.9915\n",
      "Epoch 34/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0703 - acc: 0.9829\n",
      "Epoch 35/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0751 - acc: 0.9744\n",
      "Epoch 36/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0449 - acc: 0.9915\n",
      "Epoch 37/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0848 - acc: 0.9829\n",
      "Epoch 38/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0510 - acc: 0.9915\n",
      "Epoch 39/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0522 - acc: 0.9915\n",
      "Epoch 40/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0529 - acc: 0.9829\n",
      "Epoch 41/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0707 - acc: 0.9829\n",
      "Epoch 42/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0530 - acc: 0.9829\n",
      "Epoch 43/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0521 - acc: 0.9829\n",
      "Epoch 44/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0544 - acc: 0.9915\n",
      "Epoch 45/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0430 - acc: 0.9829\n",
      "Epoch 46/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0559 - acc: 0.9915\n",
      "Epoch 47/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0480 - acc: 0.9915\n",
      "Epoch 48/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0525 - acc: 0.9829\n",
      "Epoch 49/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0438 - acc: 0.9829\n",
      "Epoch 50/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0261 - acc: 0.9915\n",
      "Epoch 51/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0733 - acc: 0.9915\n",
      "Epoch 52/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0690 - acc: 0.9829\n",
      "Epoch 53/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0199 - acc: 0.9915\n",
      "Epoch 54/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0254 - acc: 0.9915\n",
      "Epoch 55/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0792 - acc: 0.9829\n",
      "Epoch 56/150\n",
      "117/117 [==============================] - 2s 15ms/step - loss: 0.0666 - acc: 0.9829\n",
      "Epoch 57/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0692 - acc: 0.9915\n",
      "Epoch 58/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0434 - acc: 0.9915\n",
      "Epoch 59/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0333 - acc: 0.9915\n",
      "Epoch 60/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0571 - acc: 0.9915\n",
      "Epoch 61/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0423 - acc: 0.9915\n",
      "Epoch 62/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0458 - acc: 0.9829\n",
      "Epoch 63/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0326 - acc: 0.9915\n",
      "Epoch 64/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 65/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0246 - acc: 0.9915\n",
      "Epoch 66/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0202 - acc: 0.9915\n",
      "Epoch 67/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 68/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0248 - acc: 0.9915\n",
      "Epoch 69/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0114 - acc: 0.9915\n",
      "Epoch 70/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0234 - acc: 0.9915\n",
      "Epoch 71/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0142 - acc: 0.9915\n",
      "Epoch 72/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 73/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0286 - acc: 0.9915\n",
      "Epoch 74/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0129 - acc: 0.9915\n",
      "Epoch 75/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0216 - acc: 0.9915\n",
      "Epoch 76/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0087 - acc: 0.9915\n",
      "Epoch 77/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 78/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0140 - acc: 0.9915\n",
      "Epoch 79/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0110 - acc: 0.9915\n",
      "Epoch 80/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0172 - acc: 0.9915\n",
      "Epoch 81/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0166 - acc: 0.9915\n",
      "Epoch 82/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 83/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0077 - acc: 0.9915\n",
      "Epoch 85/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0138 - acc: 0.9915\n",
      "Epoch 86/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0437 - acc: 0.9915\n",
      "Epoch 87/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0861 - acc: 0.9829\n",
      "Epoch 88/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0403 - acc: 0.9915\n",
      "Epoch 89/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 90/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 91/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 92/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0189 - acc: 0.9915\n",
      "Epoch 93/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0887 - acc: 0.9829\n",
      "Epoch 94/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 95/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0070 - acc: 0.9915\n",
      "Epoch 96/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 97/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0159 - acc: 0.9915\n",
      "Epoch 98/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 99/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0279 - acc: 0.9915\n",
      "Epoch 100/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 6.2432e-04 - acc: 1.0000\n",
      "Epoch 101/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 102/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0121 - acc: 0.9915\n",
      "Epoch 103/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 6.6719e-04 - acc: 1.0000\n",
      "Epoch 104/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 105/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0414 - acc: 0.9915\n",
      "Epoch 106/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 107/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 3.2174e-04 - acc: 1.0000\n",
      "Epoch 108/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0429 - acc: 0.9915\n",
      "Epoch 109/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 110/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 8.8766e-04 - acc: 1.0000\n",
      "Epoch 111/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0231 - acc: 0.9915\n",
      "Epoch 112/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 6.7233e-04 - acc: 1.0000\n",
      "Epoch 113/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 9.9132e-04 - acc: 1.0000\n",
      "Epoch 114/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 115/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0149 - acc: 0.9915\n",
      "Epoch 116/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0267 - acc: 0.9915\n",
      "Epoch 117/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 118/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 3.5967e-04 - acc: 1.0000\n",
      "Epoch 119/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0302 - acc: 0.9915\n",
      "Epoch 120/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 2.7772e-04 - acc: 1.0000\n",
      "Epoch 121/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 122/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 123/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0067 - acc: 0.9915\n",
      "Epoch 124/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0134 - acc: 0.9915\n",
      "Epoch 125/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 126/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0338 - acc: 0.9915\n",
      "Epoch 127/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 128/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 3.6497e-04 - acc: 1.0000\n",
      "Epoch 129/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 2.1013e-04 - acc: 1.0000\n",
      "Epoch 130/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 131/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 0.0330 - acc: 0.9915\n",
      "Epoch 132/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 6.6663e-04 - acc: 1.0000\n",
      "Epoch 133/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0219 - acc: 0.9915\n",
      "Epoch 134/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 2.9293e-04 - acc: 1.0000\n",
      "Epoch 135/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 136/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 1.1203e-04 - acc: 1.0000\n",
      "Epoch 137/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 9.3072e-04 - acc: 1.0000\n",
      "Epoch 138/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0250 - acc: 0.9915\n",
      "Epoch 139/150\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 140/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 1.2920e-04 - acc: 1.0000\n",
      "Epoch 141/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 1.4494e-04 - acc: 1.0000\n",
      "Epoch 142/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 4.9339e-04 - acc: 1.0000\n",
      "Epoch 143/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 144/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 3.7562e-04 - acc: 1.0000\n",
      "Epoch 145/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 1.4639e-04 - acc: 1.0000\n",
      "Epoch 146/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 7.3688e-05 - acc: 1.0000\n",
      "Epoch 147/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 5.3966e-04 - acc: 1.0000\n",
      "Epoch 148/150\n",
      "117/117 [==============================] - 1s 13ms/step - loss: 1.1257e-04 - acc: 1.0000\n",
      "Epoch 149/150\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 1.7313e-04 - acc: 1.0000\n",
      "Epoch 150/150\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.0251 - acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131f7f080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([inputs_train, queries_train], answers_train, batch_size=1, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- 결과 도출 함수 -----\n",
    "def vocab_result(x, vocab):\n",
    "    if x.argmax() != 0: return vocab[int(x.argmax())-1]\n",
    "    else: return False\n",
    "    \n",
    "def v_s(data, s_m):\n",
    "    a = []\n",
    "    for i in data:\n",
    "        if i == None:\n",
    "            a.append(word_idx['UNK'])\n",
    "        else:\n",
    "            a.append(word_idx[i])\n",
    "    return (pad_sequences([a], maxlen=s_m))\n",
    "\n",
    "def ref_result(result1, result2, mode='detail'):\n",
    "    a = distance.euclidean(result1[0], result2[0])\n",
    "    c =  vocab_result(result2[0], vocab)\n",
    "    d = max(result2[0])\n",
    "    if c != False:\n",
    "        b = c\n",
    "    if mode == 'detail':\n",
    "        print('detail_ref / acc :', a, '/', max(result2[0]))\n",
    "    \n",
    "    if mode == 'simple' or mode == 'detail':\n",
    "        print('연관도 :', a)\n",
    "        print('정확도 :', d)\n",
    "        if b:\n",
    "            print('정답 :', ''.join(b))\n",
    "        else:\n",
    "            print('답변없음')\n",
    "    return a, d\n",
    "\n",
    "def answer_result(result1, threshold=0.9):\n",
    "    a, b = [], []\n",
    "    x = vocab_result(result1[0], vocab)\n",
    "    if x != False:\n",
    "        if max(result1[0]) > threshold:\n",
    "            a.append(x)\n",
    "            a.append(\" \")\n",
    "            a.append(max(result1[0]))\n",
    "            a.append(\" \")\n",
    "            b.append(x)\n",
    "            b.append(\" \")\n",
    "        else:\n",
    "            return False, False\n",
    "    \n",
    "    if a != []:\n",
    "        return ''.join([str(i) for i in a]), ''.join([str(i) for i in b])\n",
    "    else:\n",
    "        return False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample : [인사말] 안녕하세요. 저는 아우라봇입니다. <br/>아직 공부하고 있는 중이라 할 수 있는 건 별로 없지만 <br/>앞으로 잘 알려 주시면 열심히 공부할게요. :D <br/>지금 할 수 있는 것은 아래와 같습니다. <br/>1. 인사 <br/>2.봇 소개 <br/>3.아우라 팀 소개\n"
     ]
    }
   ],
   "source": [
    "#답변 불러오기\n",
    "with codecs.open('./answers.json', 'r', encoding='UTF-8') as json_data:\n",
    "    answers_data = json.load(json_data)['answers']\n",
    "json_data.close()\n",
    "an = 1  # 샘플로 처음 데이터를 불러옴\n",
    "print('Sample : ['+answers_data[an-1]['ID']+']', answers_data[an-1]['AN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대화 초기화\n",
    "Input_Data = []\n",
    "threshold = 0.9      #0.9\n",
    "ref_threshold = 1.5      #1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자질문 > 안녕 ?\n",
      "맥락정보 : 없음\n",
      "연관도 : 0.0\n",
      "정확도 : 0.9999999\n",
      "정답 : 인사1\n",
      "\n",
      "무맥락답변 : 인사1 0.9999999 \n",
      "최적맥락답변 : 인사1 0.9999999 \n",
      "=================================\n",
      "질문 : ['안녕', '?']\n",
      "전체맥락 : \n",
      "\u001b[1m\u001b[31m최종답변 : 안녕하세요. 아우라봇이에요 :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/GPU36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#맥락 별 중복해서 맥락 간 정확도 체크\n",
    "Q = input('사용자질문 > ')\n",
    "\n",
    "try:\n",
    "    q_data = tokenize(Q)\n",
    "    ref_data,Input_Data_A = [], []\n",
    "    m_acc = []\n",
    "    results = []\n",
    "    result_no = 0\n",
    "    final_answer = None\n",
    "    \n",
    "    #맥락 flatten\n",
    "    Input_Data_F = [y for x in Input_Data for y in x]\n",
    "    \n",
    "    #맥락 없음\n",
    "    print('맥락정보 : 없음')\n",
    "    results.append(model.predict([v_s(tokenize('UNK'), story_maxlen), v_s(q_data, query_maxlen)]))\n",
    "    _, a = ref_result(results[0], results[0], mode='simple')     #detail / simple / none\n",
    "    m_acc.append(a)\n",
    "    \n",
    "    result_no += 1\n",
    "    print()\n",
    "\n",
    "    #각 맥락 별 결과 비교값\n",
    "    j = []\n",
    "    for i in Input_Data:\n",
    "        j.extend(i)\n",
    "        print('맥락정보 :', end=\"\")\n",
    "        for f in j:\n",
    "            print(vocab[f-1], end=\" \")\n",
    "        print()\n",
    "        \n",
    "        result = model.predict([pad_sequences([j], story_maxlen, truncating='post'), v_s(q_data, query_maxlen)])\n",
    "        a, b = ref_result(results[0], result, mode='simple')     #detail / simple / none\n",
    "        \n",
    "        if a < ref_threshold:\n",
    "            results.append(result)\n",
    "            m_acc.append(b)\n",
    "            result_no += 1\n",
    "        print()\n",
    "    \n",
    "    #순위 선정하기\n",
    "    rank_1 = np.argmax(m_acc)\n",
    "\n",
    "    for i in range(rank_1):\n",
    "        ref_data.append(Input_Data[i])\n",
    "\n",
    "    #무맥락 답변\n",
    "    print('무맥락답변 :', end=' ')\n",
    "    an_no, f_an_no = answer_result(results[0], threshold=threshold)\n",
    "    if an_no == False:\n",
    "        an_no, f_an_no = '무슨 뜻인지 모르겠어요', '무슨 뜻인지 모르겠어요'\n",
    "    print(an_no)\n",
    "\n",
    "    #최적맥락 답변\n",
    "    print('최적맥락답변 :', end=' ')\n",
    "    an_A, f_an_A = answer_result(results[rank_1], threshold=threshold)\n",
    "    \n",
    "    if an_A == False:\n",
    "        an_A, f_an_A = '무슨 뜻인지 모르겠어요', '무슨 뜻인지 모르겠어요'\n",
    "    print(an_A)\n",
    "\n",
    "    if ref_data == []:\n",
    "        final_answer = f_an_no\n",
    "    else:\n",
    "        final_answer = f_an_A\n",
    "\n",
    "    #이전 대화 저장하기\n",
    "    x_d = []\n",
    "    for i in v_s(q_data, query_maxlen):\n",
    "        for j in reversed(i):\n",
    "            if j != 0:\n",
    "                x_d.insert(0, j)\n",
    "\n",
    "    Input_Data = ref_data + [x_d]\n",
    "\n",
    "    #종합\n",
    "    print(\"=================================\")\n",
    "    print('질문 :',q_data)\n",
    "    print('전체맥락 :', end=\" \")\n",
    "    for i in Input_Data_F:\n",
    "        print(vocab[i-1], end=\" \")\n",
    "    print()\n",
    "    for i in answers_data:\n",
    "        if i['ID'] == final_answer.strip():\n",
    "            final_answer = i['AN']      \n",
    "    print(\"\\033[1m\\033[31m최종답변 :\", final_answer)\n",
    "     \n",
    "except KeyError:\n",
    "    print('※ 사전에 있는 단어를 입력해 주세요.')\n",
    "    print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
